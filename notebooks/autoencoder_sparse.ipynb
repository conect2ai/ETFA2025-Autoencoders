{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras import layers, models, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/normalized_train.csv\")\n",
    "df_val = pd.read_csv(\"../data/normalized_val.csv\")\n",
    "df_test = pd.read_csv(\"../data/normalized_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(models.Model):\n",
    "    def __init__(self, input_shape, bottleneck_dim, encoder_layers, decoder_layers, sparsity_param=0.05, beta=1e-3):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.sparsity_param = sparsity_param\n",
    "        self.beta = beta\n",
    "\n",
    "        self.encoder = models.Sequential(name=\"encoder\")\n",
    "        self.encoder.add(layers.Input(shape=(input_shape,)))\n",
    "\n",
    "        for units in encoder_layers:\n",
    "            self.encoder.add(layers.Dense(units, activation=\"relu\", dtype=\"float32\"))\n",
    "            self.encoder.add(layers.BatchNormalization(dtype=\"float32\"))\n",
    "\n",
    "        self.bottleneck = layers.Dense(bottleneck_dim, activation=\"sigmoid\", activity_regularizer=regularizers.l1(1e-5), dtype=\"float32\", name=\"bottleneck\")\n",
    "        self.encoder.add(self.bottleneck)\n",
    "\n",
    "        self.decoder = models.Sequential(name=\"decoder\")\n",
    "        self.decoder.add(layers.Input(shape=(bottleneck_dim,)))\n",
    "\n",
    "        for units in decoder_layers:\n",
    "            self.decoder.add(layers.Dense(units, activation=\"relu\", dtype=\"float32\"))\n",
    "            self.decoder.add(layers.BatchNormalization(dtype=\"float32\"))\n",
    "\n",
    "        self.decoder.add(layers.Dense(input_shape, activation=\"linear\", dtype=\"float32\"))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        decoded = self.decoder(encoded, training=training)\n",
    "\n",
    "        self.add_loss(self.sparsity_loss(encoded))\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return tf.keras.models.Model(inputs=self.encoder.inputs, outputs=self.encoder.get_layer(\"bottleneck\").output)\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    def sparsity_loss(self, encoded):\n",
    "        rho_hat = tf.reduce_mean(encoded, axis=0)\n",
    "        kl_div = tf.reduce_sum(\n",
    "            self.sparsity_param * tf.math.log(self.sparsity_param / (rho_hat + 1e-10)) +\n",
    "            (1 - self.sparsity_param) * tf.math.log((1 - self.sparsity_param) / (1 - rho_hat + 1e-10))\n",
    "        )\n",
    "        return self.beta * kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_config(config, df_train, df_val, df_test, input_dim=15):\n",
    "    print(f\"Treinando configuração: {config['name']}\\n\")\n",
    "\n",
    "    total_results = []\n",
    "\n",
    "    for latent_dim in range(input_dim, 0, -1):\n",
    "        print(f\"Treinando modelo com espaço latente = {latent_dim}\\n\")\n",
    "\n",
    "        autoencoder = SparseAutoencoder(\n",
    "            input_shape=input_dim,\n",
    "            bottleneck_dim=latent_dim,\n",
    "            encoder_layers=config[\"encoder_layers\"],\n",
    "            decoder_layers=config[\"decoder_layers\"],\n",
    "            sparsity_param=0.05,\n",
    "            beta=1e-3\n",
    "        )\n",
    "\n",
    "        autoencoder.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            loss=\"mse\"\n",
    "        )\n",
    "\n",
    "        lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, min_lr=1e-5)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
    "\n",
    "        history = autoencoder.fit(\n",
    "            df_train.drop(columns=[\"FuelType\"]).values.astype(\"float32\"),\n",
    "            df_train.drop(columns=[\"FuelType\"]).values.astype(\"float32\"),\n",
    "            epochs=1000,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            validation_data=(\n",
    "                df_val.drop(columns=[\"FuelType\"]).values.astype(\"float32\"),\n",
    "                df_val.drop(columns=[\"FuelType\"]).values.astype(\"float32\")\n",
    "            ),\n",
    "            callbacks=[lr_schedule, early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        num_epochs = len(history.history[\"loss\"])\n",
    "\n",
    "        test_reconstructed = autoencoder.predict(df_test.drop(columns=[\"FuelType\"]).values.astype(\"float32\"))\n",
    "        mse = np.mean((df_test.drop(columns=[\"FuelType\"]).values.astype(\"float32\") - test_reconstructed) ** 2)\n",
    "\n",
    "        encoder = autoencoder.get_encoder()\n",
    "        model_path = f\"results/{config['name']}_latent_{latent_dim}\"\n",
    "        encoder.save(f\"{model_path}_encoder.keras\")\n",
    "\n",
    "        encoder_size = os.path.getsize(f\"{model_path}_encoder.keras\") / 1024\n",
    "\n",
    "        result = {\n",
    "            \"Configuração\": config['name'],\n",
    "            \"Espaço Latente\": latent_dim,\n",
    "            \"Tamanho Encoder (KB)\": encoder_size,\n",
    "            \"Número de Épocas\": num_epochs,\n",
    "            \"Erro MSE\": mse\n",
    "        }\n",
    "\n",
    "        pd.DataFrame(result, index=[0]).to_csv(f\"{model_path}.csv\", index=False)\n",
    "\n",
    "        total_results.append(result)\n",
    "\n",
    "        print(f\"Modelo com latente={latent_dim} salvo!\\n\")\n",
    "\n",
    "    pd.DataFrame(total_results).to_csv(f\"results/{config['name']}_summary.csv\", index=False)\n",
    "\n",
    "    print(\"\\nTreinamento concluído para todas as configurações!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symmetrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"sparse_autoencoder_symmetric_c1\",\n",
    "    \"encoder_layers\": [128, 64, 32],\n",
    "    \"decoder_layers\": [32, 64, 128]\n",
    "}\n",
    "\n",
    "train_models_for_config(config, df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"sparse_autoencoder_symmetric_c2\",\n",
    "    \"encoder_layers\": [256, 128, 64],\n",
    "    \"decoder_layers\": [64, 128, 256]\n",
    "}\n",
    "\n",
    "train_models_for_config(config, df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asymmetrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"sparse_autoencoder_asymmetric_c1\",\n",
    "    \"encoder_layers\": [128, 64],\n",
    "    \"decoder_layers\": [64, 128, 256]\n",
    "}\n",
    "\n",
    "train_models_for_config(config, df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"sparse_autoencoder_asymmetric_c2\",\n",
    "    \"encoder_layers\": [128, 64],\n",
    "    \"decoder_layers\": [64, 128, 256, 512]\n",
    "}\n",
    "\n",
    "train_models_for_config(config, df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"sparse_autoencoder_asymmetric_c3\",\n",
    "    \"encoder_layers\": [256, 128],\n",
    "    \"decoder_layers\": [128, 256, 512]\n",
    "}\n",
    "\n",
    "train_models_for_config(config, df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"sparse_autoencoder_asymmetric_c4\",\n",
    "    \"encoder_layers\": [256, 128],\n",
    "    \"decoder_layers\": [128, 256, 512, 1024]\n",
    "}\n",
    "\n",
    "train_models_for_config(config, df_train, df_val, df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etfa2025-autoencoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
